---
title: What is chunking and why?
---

import imageUrl from '../../tmp_images/academy_placeholder.jpg';

<img src={imageUrl} alt="Image alt" width="75%"/>

## <i class="fa-solid fa-square-chevron-right"></i>&nbsp;&nbsp;What is chunking?

In the context of natural language processing, "chunking" refers to the process of splitting texts into smaller pieces of texts, i.e. "chunks".

For reasons that we'll discuss soon, chunking is a very common and important pre-processing step both in the context of vector databases as well as large language models (LLMs).

## <i class="fa-solid fa-square-chevron-right"></i>&nbsp;&nbsp;Why chunk data?

There are many potential reasons for chunking data, but for vector databases and LLMs, there are important, but different reasons for doing so.

### <i class="fa-solid fa-chalkboard"></i>&nbsp;&nbsp;Information retrieval

For vector databases (and therefore vector search), chunking has the effect of defining a unit of information to be retrieved. A chunk will become a smallest quantum of information to be catalogued or retrieved by the vector database.

So, it is very important to get chunk sizing right for your use case. Failure modes can include chunks being too small, or chunks being too big.

:::warning TODO
Add conceptual figure showing big/small chunks
:::

#### (Too) Small chunks

If chunks are too small, each text object may not contain enough information or context for meaningful search to be performed.

Imagine a book that is chunked into individual words and vectorized.

These vectors would be likely too granular to provide any kind of meaningful, contextual results in a search. (Unless you were using it as a thesaurus!)

Note that chunks that are a few words may be okay in some situations. Such as when you are looking for a specific phrase, or information.

:::warning TODO
ADD FIGURES OF SMALL CARDS SHOWING WORD-SIZED CHUNKS
:::

#### (Too) Large chunks

On the other hand, if chunks are too large, each chunk could not be meaningfully represented by a vector embedding.

Imagine a book that is saved as one "chunk", or even chapters. Is this a good idea?

Such an arrangement would not be suitable if what you were after was discovery of relevant snippets, or sections. But it may be reasonable, if you are looking to find the right chapter, or section, in - say - a library of books.

:::warning TODO
ADD FIGURES OF SMALL CARDS SHOWING WORD-SIZED CHUNKS
:::

Look at it this way - a vector embedding is a representation of the underlying object's "meaning". And a vector database catalogs those meanings so that they can be searched.

Accordingly, each chunk needs to be an appropriate size that the captured meaning reflects the kind of search to be performed.

### <i class="fa-solid fa-chalkboard"></i>&nbsp;&nbsp;Vectorization token limit

Another need for chunking comes from the vectorization process.

Depending on the model used for vectorization, the text length may exceed the maximum input length. This is especially true for modern models, as their transformer-based architecture can lead to high resource requirements (and thus shorter maximum lengths).


### <i class="fa-solid fa-chalkboard"></i>&nbsp;&nbsp;Context window limit

Chunking is also often necessary for using large language models, for two reasons.

One is to ensure that the input does not exceed the maximum allowable input length, typically called the "context window".

The second is to determine:

- How many chunks can be passed onto the LLM, and
- The amount of context that is passed onto the LLM per chunk.

:::warning TODO
Add conceptual figure showing big/small chunks for RAG
:::

#### (Too) Small chunks

Using short chunks, you can add information from more chunks to the LLM. However, it may lead to insufficient contextual information being passed on in each result to the LLM.

#### (Too) Large chunks

On the other hand, using larger chunk sizes would reduce the number of objects that can be passed to the LLM. This may adversely impact the diversity of the input information, although they will include more contextual information per object.

## <i class="fa-solid fa-square-chevron-right"></i>&nbsp;&nbsp;Chunk size selection

As you can start to see, there are multiple factors at play to help you choose the right chunk size.

Unfortunately, there isn't a chunk size or chunking technique that works for everybody. The trick here is to find a size that works for *you* - one that isn't too small or too large, and also chunked with a method that suits you.

In the next unit, we'll begin to review these ideas, starting with some common chunking techniques.

## <i class="fa-solid fa-square-chevron-right"></i>&nbsp;&nbsp;Review

<Quiz questions={varName} />

Any quiz questions

### <i class="fa-solid fa-pen-to-square"></i>&nbsp;&nbsp;Review exercise

:::note <i class="fa-solid fa-square-terminal"></i> Exercise
Try out ...
:::

### <i class="fa-solid fa-lightbulb-on"></i>&nbsp;&nbsp;Key takeaways

:::info
Add summary
:::

import { GiscusDocComment } from '/src/components/GiscusComment';

<GiscusDocComment />

import Quiz from '/src/components/Academy/quiz.js'
const varName = [{
  questionText: 'questionText',
  answerOptions: [
    {
      answerText: 'answerOne',
      isCorrect: false,
      feedback: 'feedbackOne',
    },
    {
      answerText: 'answerTwo',
      isCorrect: false,
      feedback: 'feedbackTwo',
    },
    {
      answerText: 'answerThree',
      isCorrect: false,
      feedback: 'feedbackThree',
    },
  ]
}];