---
title: A brief introduction to chunking
---

import imageUrl from '../../tmp_images/academy_placeholder.jpg';

<img src={imageUrl} alt="Image alt" width="75%"/>

## <i class="fa-solid fa-square-chevron-right"></i>&nbsp;&nbsp;What is chunking?

Chunking is the process of splitting texts into smaller pieces of texts, i.e. "chunks". Consider a case where the source text comprises a set of books. A chunking method could conceivably split the text into a set of chapters, paragraphs, sentences, or even words, into chunks.

While this is a simple concept, it can have a significant impact on the performance of vector databases, and outputs of language models.

## <i class="fa-solid fa-square-chevron-right"></i>&nbsp;&nbsp;Why chunk data?

### <i class="fa-solid fa-chalkboard"></i>&nbsp;&nbsp;For information retrieval

You know by now that a vector database stores objects by corresponding vectors to capture their meaning. But how much text each vector capture?

Chunking defines this. Each chunk is the unit of information that vectorized and stored in the database.

Let's go back to the above example, building a vector database from a set of books.

At one extreme, you could catalog each book as one vector. This would build a database akin to a library catalog. Each search would do a good job of finding the right book. But it would not be so good for finding specific information within a book.

At the other extreme, cataloging each sentence as a vector would build a database akin to a (sentence-level) thesaurus. This would be good for finding specific concepts or information conveyed by the writer. But it would not be so good for finding broader information, such as the idea conveyed by a book, or even a chapter.

Whether to choose one or the other approach, or a third approach in-between, depends on your use case. But the key point is that chunking defines the unit of information that is stored in the database, and therefore each unit of information to be retrieved.

As we will see later on, this has implications not only search, but also retrieval augmented generation (RAG) use case downstream.

### <i class="fa-solid fa-chalkboard"></i>&nbsp;&nbsp;To meet model requirements

In fact, text data can often be very long. Think about lengths of articles, transcripts, or even books. Instead of a few words, these texts can be thousands, or tens of thousands of words long if not longer. *The Lord of the Rings*, for example, is over 500,000 words long!

That's great for readers, but less great for language models. Long texts like these can pose problems to modern language models. Thse models typically have a finite "window" of text input lengths, and source texts often exceed this length.

To deal with this limitation, a technique called chunking is often used. Like the name suggests, chunking splits longer bodies of text into smaller portions that can be handled by the model. However, there are many ways to chunk a text, and the choice of chunking strategy can have a significant impact on the performance of your model.


There are many potential reasons for chunking data, but for vector databases and LLMs, there are important, but different reasons for doing so.

At the core, this is a simple concept. But there are many ways to chunk a text, and the choice of chunking strategy can have a significant impact on the final results. But before we get all that - let's start with the basics. Why do we need to chunk data in the first place when we're dealing with vector databases and language models?



### <i class="fa-solid fa-chalkboard"></i>&nbsp;&nbsp;Information retrieval



### <i class="fa-solid fa-chalkboard"></i>&nbsp;&nbsp;Vectorization token limit

Another need for chunking comes from the vectorization process.

Depending on the model used for vectorization, the text length may exceed the maximum input length. This is especially true for modern models, as their transformer-based architecture can lead to high resource requirements (and thus shorter maximum lengths).


### <i class="fa-solid fa-chalkboard"></i>&nbsp;&nbsp;Context window limit

Chunking is also often necessary for using large language models, for two reasons.

One is to ensure that the input does not exceed the maximum allowable input length, typically called the "context window".

The second is to determine:

- How many chunks can be passed onto the LLM, and
- The amount of context that is passed onto the LLM per chunk.

:::warning TODO
Add conceptual figure showing big/small chunks for RAG
:::

#### (Too) Small chunks

Using short chunks, you can add information from more chunks to the LLM. However, it may lead to insufficient contextual information being passed on in each result to the LLM.

#### (Too) Large chunks

On the other hand, using larger chunk sizes would reduce the number of objects that can be passed to the LLM. This may adversely impact the diversity of the input information, although they will include more contextual information per object.

## <i class="fa-solid fa-square-chevron-right"></i>&nbsp;&nbsp;Chunk size selection

As you can start to see, there are multiple factors at play to help you choose the right chunk size.

Unfortunately, there isn't a chunk size or chunking technique that works for everybody. The trick here is to find a size that works for *you* - one that isn't too small or too large, and also chunked with a method that suits you.

In the next unit, we'll begin to review these ideas, starting with some common chunking techniques.

## <i class="fa-solid fa-square-chevron-right"></i>&nbsp;&nbsp;Review

<Quiz questions={varName} />

Any quiz questions

### <i class="fa-solid fa-pen-to-square"></i>&nbsp;&nbsp;Review exercise

:::note <i class="fa-solid fa-square-terminal"></i> Exercise
Try out ...
:::

### <i class="fa-solid fa-lightbulb-on"></i>&nbsp;&nbsp;Key takeaways

:::info
Add summary
:::

import { GiscusDocComment } from '/src/components/GiscusComment';

<GiscusDocComment />

import Quiz from '/src/components/Academy/quiz.js'
const varName = [{
  questionText: 'questionText',
  answerOptions: [
    {
      answerText: 'answerOne',
      isCorrect: false,
      feedback: 'feedbackOne',
    },
    {
      answerText: 'answerTwo',
      isCorrect: false,
      feedback: 'feedbackTwo',
    },
    {
      answerText: 'answerThree',
      isCorrect: false,
      feedback: 'feedbackThree',
    },
  ]
}];