---
title: Using Embedding models in Weaviate
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import FilteredTextBlock from '@site/src/components/Documentation/FilteredTextBlock';
import TSCode from '!!raw-loader!./_snippets/10_embedding.ts';
import EmbeddingModelImage from '/developers/academy/js/standalone/using-ml-models/_img/embedding-models.jpg';
import Unimodal from '/developers/academy/js/standalone/using-ml-models/_img/unimodal.jpg';
import Multimodal from '/developers/academy/js/standalone/using-ml-models/_img/multimodal.jpg';


## <i class="fa-solid fa-chalkboard"></i> What are Embedding Models

Embedding models are machine learning models trained to represent information as an array of numbers, frequently referred to as vector embeddings. Vectors or vector embeddings are numeric representations of data that represent certain properties or features. This representation can be used to efficiently search through objects in a vector space. 

<img src={EmbeddingModelImage} alt="Image alt" width="100%"/>


## <i class="fa-solid fa-chalkboard"></i> When to use Embedding Models

Embeddings are the worker horses behind modern search and Retrieval-Augmented Generation (RAG) applications. They are great for..
 
- **Search:** Results of searches are ranked by the distance from an input query vector. 
- **Classification:** Items are classified by what category their vector representation is closest to.
- **Recommendations:** Items with similar vector representations are recommended to users.


## <i class="fa-solid fa-chalkboard"></i> Applications of Embedding Models

Embedding models, like most machine learning models are typically limited to one or more modalities. 

We use modality to describe the type of input or output that a machine learning model can process or interact with to run. Typically, embedding modals fall into two buckets, uni-modal or multimodal. 


- **Uni-modal Embeddings**: These embeddings represents a single modality in a multi-dimensional vector space. Examples of these are [embed-multilingual-v3.0](https://cohere.com/blog/introducing-embed-v3) a text embedding model by Cohere or [marengo 2.7](https://www.twelvelabs.io/blog/introducing-marengo-2-7) a video embedding models by Twelve Labs.

<img src={Unimodal} alt="Image alt" width="90%" style={{ 
      marginRight: 0,
      marginLeft: 'auto',
      display: 'block'
    }}/>


- **Multimodal Embeddings**: These embeddings represent multiple modalities in a multi-dimensional space. Allowing cross modal retrieval and clustering. [CLIP](https://openai.com/index/clip/) is a popular multimodal model that can create embeddings of text, audio and video data.

<img src={Multimodal} alt="Image alt" width="90%" style={{ 
      marginRight: 0,
      marginLeft: 'auto',
      display: 'block'
    }}/>



## <i class="fa-solid fa-chalkboard"></i> Using Embedding Models in Weaviate

Weaviate takes most of the complexity of generating and managing embeddings away! Weaviate is configured to support many different vectorizer models and vectorizer service providers. It also gives you the option of providing your own vectors.

In Weaviate, vector embeddings power hybrid and semantic search. 

Lets walk through the process to configure embedding models in Weaviate and make a semantic search. We'll start by creating a free sandbox account on [Weaviate Cloud](https://console.weaviate.cloud/). Follow [this guide](https://weaviate.io/developers/wcs/connect) if you have trouble setting up a sandbox project.

### Step 1: Connect to a Weaviate instance

 <TabItem value="js" label="app.js">
    <FilteredTextBlock
      text={TSCode}
      startMarker="// START Connect"
      endMarker="// END Connect"
      language="js"
    />
  </TabItem>

Initialize your connection with Weaviate and add relevant environment variables necessary to access third party embedding models.

### Step 2: Define a Collection and Embedding Model 

 <TabItem value="js" label="app.js">
    <FilteredTextBlock
      text={TSCode}
      startMarker="// START Collection"
      endMarker="// END Collection"
      language="js"
    />
  </TabItem>

When creating a collection in Weaviate, we define what embedding model we want to use. In this example we use a  text embedding model by Cohere to create vector embeddings our data. This is **embed-multilingual-v3.0** when we use the `text2vecCohere()` module. 


### Step 3: Importing data 

 <TabItem value="js" label="app.js">
    <FilteredTextBlock
      text={TSCode}
      startMarker="// START Importing"
      endMarker="// END Importing"
      language="js"
    />
  </TabItem>

Once our collection is created, we import data. It is at import time where we interact with our embedding model. The Weaviate vectorizer sends objects to the embedding model we define during collection creation. At the end of this, we have both our data objects and their corresponding vector representations stored in our Vector Database. Now we can run semantic search queries.


### Step 4: Running a Semantic Search

 <TabItem value="js" label="app.js">
    <FilteredTextBlock
      text={TSCode}
      startMarker="// START Search"
      endMarker="// END Search"
      language="js"
    />
  </TabItem>


Here we make a query and set `return` as true so we can see the objects' vectors in our response. Read more about [search here](https://weaviate.io/developers/weaviate/concepts/search).




## <i class="fa-solid fa-chalkboard"></i> Bonus Learning

### Vector Representations
Vector representations are the fundamental output of embedding models. They translate complex data (text, images, etc.) into fixed-length arrays of numbers that capture semantic meaning.

- **Dimensionality**: Typically ranges from 384 to 1536 dimensions, depending on the model. A larger dimensionality usually means more accuracy but also a higher memory footprint for generated vectors. 

- **Format**: Vectors are typically floating point numbers, usually normalized to a specific range.

### Distance metrics

[Distance metrics](https://weaviate.io/developers/weaviate/config-refs/distances) quantify the similarity between vector embeddings. Weaviate uses **[cosine similarity](https://weaviate.io/blog/distance-metrics-in-vector-search#cosine-similarity)** as the default distance metric for semantic similarity.


<!-- <img src={EmbeddingModelImage} alt="Image alt" width="90%"/> -->


Embedding models vary when it comes to performance and ability, [read through this article](https://weaviate.io/blog/how-to-choose-an-embedding-model) so you have an idea of what to think about decide between various models. 

