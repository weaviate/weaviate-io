---
title: Chunking techniques - 1
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import FilteredTextBlock from '@site/src/components/Documentation/FilteredTextBlock';
import CodeFixedSizeChunking from '!!raw-loader!./_snippets/20_chunking_methods.1.fixed.size.py';
import CodeVariableSizeChunking from '!!raw-loader!./_snippets/20_chunking_methods.2.variable.size.py';
import CodeMixedStrategyChunking from '!!raw-loader!./_snippets/20_chunking_methods.3.mixed.strategy.py';

<!-- import imageUrl from '../../tmp_images/academy_placeholder.jpg';

<img src={imageUrl} alt="Image alt" width="75%"/> -->

import PreviewUnit from '../../../_snippets/preview.mdx'

<PreviewUnit />

## <i class="fa-solid fa-square-chevron-right"></i> Overview

Now that you've learned about what chunking is, and why it is important, you are ready to start looking at practical chunking techniques. Here, we start by looking at **fixed-size** chunking techniques, including some example implementations.

## <i class="fa-solid fa-square-chevron-right"></i> Fixed-size chunking

As the name suggests, fixed-size chunking refers to the process of splitting texts into chunks of a fixed size, or at least based on size. Using fixed size chunking, you might split an article into a set of chunks of 100 words each, or a set of 200 characters each.

This may be the most common chunking technique due to its simplicity and effectiveness.

### <i class="fa-solid fa-chalkboard"></i> Implementations

Fixed-size chunking is implemented by splitting texts into chunks of a fixed number of units. The units may be composed of words, characters, or even *tokens*, and the number of units per chunk is fixed (to a maximum), with an optional overlap.

:::tip What is a token?
A "token" in this context is a unit of text that will be processed by a model by being substituted with a number. In modern tranformer models, a token is commonly a "subword" unit composed of a few characters.
:::

One pseudocode implementation of fixed-size chunking is:

```python
# Given a text of length L
# Split the text into chunks of size N units (e.g. tokens, characters, words)
# Optionally, add an overlap of M units at the beginning or end of each chunk (from the previous or next chunk)
# This should typically result in a list of chunks of length L // N + 1
```

And implementing in Python, it may look like:

<Tabs groupId="languages">
<TabItem value="py" label="Python">
<FilteredTextBlock
  text={CodeFixedSizeChunking}
  startMarker="# START Vanilla fixed size chunker"
  endMarker="# END Vanilla fixed size chunker"
  language="py"
/>
</TabItem>
</Tabs>

Which can be modified to include an overlap (in this case, at the beginning of each chunk):

<Tabs groupId="languages">
<TabItem value="py" label="Python">
<FilteredTextBlock
  text={CodeFixedSizeChunking}
  startMarker="# START Fixed size chunker with overlap"
  endMarker="# END Fixed size chunker with overlap"
  language="py"
/>
</TabItem>
</Tabs>

This is far from the only way to implement fixed-size chunking, but it is one possible, relatively simple, implementation.

:::note Exercise
Consider how *you* might implement fixed-size chunking. What would your pseudocode (or code) look like?
:::

### <i class="fa-solid fa-code"></i> Examples

We are ready to look at some concrete examples of fixed-size chunking. Let's take a look at three examples, with a chunk size of 5 words, 25 words and 100 words, respectively.

We'll use an excerpt from the [Pro Git book](https://git-scm.com/book/en/v2)*. More specifically, we'll use text of the [What is Git?](https://github.com/progit/progit2/blob/main/book/01-introduction/sections/what-is-git.asc) chapter.

Here is one example using our chunking function from above:

<Tabs groupId="languages">
<TabItem value="py" label="Python">
<FilteredTextBlock
  text={CodeFixedSizeChunking}
  startMarker="# START Get fixed-size chunks examples"
  endMarker="# END Get fixed-size chunks examples"
  language="py"
/>
</TabItem>
</Tabs>

This will result in outputs like these. Take a look at the first few chunks at each size - what do you notice?

:::note Exercise
Consider which of these chunk sizes would be most appropriate for search. Why do you think so? What are the tradeoffs?
:::

<Tabs groupId="languages">
<TabItem value="5" label="By 5 words">
<FilteredTextBlock
  text={CodeFixedSizeChunking}
  startMarker="# START Chunking by 5 words - outputs"
  endMarker="# END Chunking by 5 words - outputs"
  language="text"
/>
</TabItem>
<TabItem value="25" label="By 25 words">
<FilteredTextBlock
  text={CodeFixedSizeChunking}
  startMarker="# START Chunking by 25 words - outputs"
  endMarker="# END Chunking by 25 words - outputs"
  language="text"
/>
</TabItem>
<TabItem value="100" label="By 100 words">
<FilteredTextBlock
  text={CodeFixedSizeChunking}
  startMarker="# START Chunking by 100 words - outputs"
  endMarker="# END Chunking by 100 words - outputs"
  language="text"
/>
</TabItem>
</Tabs>


Hopefully, these concrete examples start to illustrate some of the ideas that we discussed above.

Immediately, it strikes me that the smaller chunks are very granular, to the point where they may not contain enough information to be useful for search. On the other hand, the larger chunks begin to retain more information as they get to lengths that are similar to a typical paragraph.

Now imagine these chunks becoming even longer. As chunks become longer, the corresponding vector embeddings would start to become more general. This would eventually reach a point where they cease to be useful in terms of searching for information.

:::note What about character or sub-word tokenization?
At these sizes, you typically will not need to employ character-based or sub-word token-based chunking, as splitting words at these boundaries in a group of words will not typically be meaningful.
:::

:::tip Where to start?
For search with fixed-size chunks, if you don't have any other factors, try a size of around 100-200 words, and a 20% overlap.
:::

## <i class="fa-solid fa-square-chevron-right"></i> Notes

:::info Pro Git by Scott Chacon and Ben Straub - Book License

<small>*Available through the <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/">Creative Commons Attribution-Non Commercial-Share Alike 3.0 license</a>.</small>

:::

<!-- ## <i class="fa-solid fa-square-chevron-right"></i> Review

<Quiz questions={varName} />

Any quiz questions

### <i class="fa-solid fa-pen-to-square"></i> Review exercise

:::note <i class="fa-solid fa-square-terminal"></i> Exercise
Try out ...
:::

### <i class="fa-solid fa-lightbulb-on"></i> Key takeaways

:::info
Add summary
::: -->

import { GiscusDocComment } from '/src/components/GiscusComment';

<GiscusDocComment />

<!-- import Quiz from '/src/components/Academy/quiz.js'
const varName = [{
  questionText: 'questionText',
  answerOptions: [
    {
      answerText: 'answerOne',
      isCorrect: false,
      feedback: 'feedbackOne',
    },
    {
      answerText: 'answerTwo',
      isCorrect: false,
      feedback: 'feedbackTwo',
    },
    {
      answerText: 'answerThree',
      isCorrect: false,
      feedback: 'feedbackThree',
    },
  ]
}]; -->