---
title: Common model types
sidebar_position: 2
---

## <i class="fa-solid fa-square-chevron-right"></i> Types of vector representations

:::warning TODO
Intro video here
:::

When we talk about vectors, we are typically referring to vectors that are derived by machine-learning models. More specifically, we refer to vectors that are derived from neural networks, called "dense" vectors.

However, there are other vector representations that are used to represent meaning, especially in relation to textual meaning. They include:

- One-hot encoding
- TF-IDF (term frequency-inverse document frequency) vectors
- BM25 vectors

Let's take a brief look at each one, as well as dense vectors.

### One-hot encoding

One-hot encodings represent text as a collection of 0s and 1s, where each 1 represents the presence of a word in the text. Sometimes this is also referred to as a "bag of words" representation.

Accordingly, this representation ends up being very sparse, with most of the vector being 0s. This is because most words are not present in a given text.

A limitation of this method is that it is not able to capture similarity of words, as each word is simply represented as being present or not present. Additionally, it is not able to take into account the relative importance of words in a text.

### TF-IDF vectors

A TF-IDF representation improves on the one-hot encoding by taking into account the relative importance of words in a text.

TF-IDF stands for "term frequency-inverse document frequency". It is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.

The TF-IDF value increases proportionally to the number of times a word appears in the document, but is offset by the frequency of the word in the corpus. This means that common words across all documents such as "the", "a", "is" and "are" are penalized, whereas words that are rare across all documents are given more weight.

Intuitively, this means that TF-IDF is able to capture the relative importance of words in a text by weighting rare words more heavily.

### BM25 vectors

BM25 vectors are similar to TF-IDF vectors, but they are able to take into account the length of a document. This is important because longer documents will have more words, and therefore a higher TF-IDF score, even if they are not more relevant than shorter documents.

Accordingly, BM25 vectors normalize the TF-IDF score by the length of the document.

### Dense Vectors

#### Word vectors

Word vectors are a type of vector representation that is derived from neural networks. They are able to capture the meaning of words by learning the context in which they appear.

Models such as "word2vec" and "GloVe" popularized this type of representational learning. One key shortcoming of word-based vectors is that they are not able to take into account local context, such as the context of a word as a part of its parent sentence.

This meant that where words need to be disambiguated, such as in the case of homonyms, word vectors were not able to capture the meaning of the word in the context of the sentence. (For example, the word "bank" can mean a financial institution or a river bank.)

:::note Word vectors + weighting
Word vectors in a text can be combined with a weighting method such as TF-IDF or BM25 to capture the relative importance of words in the text. The resulting vector can be used to represent the entire text.
:::

#### Transformer-derived vectors

Most modern vector database use vectors that are derived from what are called "transformer" models.

Transformers are a type of neural network that are able to take into account the context of its parent sentence in determining the meaning of each word. This means that they are able to disambiguate words that have multiple meanings, such as the word "bank" in the example above.

Their current key shortcoming is their resource-intensive nature, especially as the input size (e.g. text length) increases.

<Quiz questions={sparseOrDense} />

<Quiz questions={wordVecVsTransformer} />

## <i class="fa-solid fa-square-chevron-right"></i> By media type

## <i class="fa-solid fa-square-chevron-right"></i> Text vectorizer types

## <i class="fa-solid fa-square-chevron-right"></i> Multi-media vectorizers

import Quiz from '/src/components/Academy/quiz.js'
const sparseOrDense = [
  {
    questionText: 'From the folloowing, select the correct statement about sparse and dense vectors.',
    answerOptions: [
      { answerText: 'One-hot encoding & word vectors: sparse, Transformer-derived: dense.', isCorrect: false, feedback: 'Word vectors are not sparse.'},
      { answerText: 'Document vector generated from BM25-weighted word vectors: sparse.', isCorrect: false, feedback: 'Word vectors are dense. Accordingly, a document vector generated by weighting BM25 scores are also dense.'},
      { answerText: 'One-hot encoding: sparse, TF-IDF based bag of words: dense.', isCorrect: false, feedback: 'Bag-of-words vectors are sparse. Accordingly, a vector that is based on TF-IDF weighting is also sparse.'},
      { answerText: 'One-hot encoding: sparse, Word vectors & transformer-derived: dense.', isCorrect: true, feedback: 'This is the only correct answer.'},
    ],
  },
];
const wordVecVsTransformer = [
  {
    questionText: 'Select the correct statement.',
    answerOptions: [
      { answerText: 'One-hot encoding & word vectors: sparse, Transformer-derived: dense.', isCorrect: false, feedback: 'Word vectors are not sparse.'},
      { answerText: 'Document vector generated from BM25-weighted word vectors: sparse.', isCorrect: false, feedback: 'Word vectors are dense. Accordingly, a document vector generated by weighting BM25 scores are also dense.'},
      { answerText: 'One-hot encoding: sparse, TF-IDF based bag of words: dense.', isCorrect: false, feedback: 'Bag-of-words vectors are sparse. Accordingly, a vector that is based on TF-IDF weighting is also sparse.'},
      { answerText: 'One-hot encoding: sparse, Word vectors & transformer-derived: dense.', isCorrect: true, feedback: 'This is the only correct answer.'},
    ],
  },
];