---
title: Wikipedia with custom vectors
sidebar_position: 50
image: og/docs/tutorials.jpg  # TODO
# tags: ['import']
---

## Overview

This tutorial will show you how to import a large dataset (25k articles from Wikipedia) that already includes vectors (embeddings generated by OpenAI). We will,
* download and unzip a CSV file that contains the Wikipedia articles
* create a Weaviate instance
* create a schema
* parse the file and batch import the records, with Python and JavaScript code
* make sure the data was imported correctly
* run a few queries to demonstrate semantic search capabilities


## Prerequisites

import BasicPrereqs from '/_includes/prerequisites-quickstart.md';

<BasicPrereqs />

Before you start this tutorial, make sure to have:

- An [OpenAI API key](https://platform.openai.com/account/api-keys). Even though we already have vector embeddings generated by OpenAI, we'll need an OpenAI key to vectorize search queries, and to recalculate vector embeddings for updated object contents.
- Your preferred Weaviate [client library](../client-libraries/index.md) installed.

<details>
  <summary>
    See how to delete data from previous tutorials (or previous runs of this tutorial).
  </summary>

import CautionSchemaDeleteClass from '/_includes/schema-delete-class.mdx'

<CautionSchemaDeleteClass />

</details>


## Download the dataset

We will use this [Simple English](https://simple.wikipedia.org/wiki/Simple_English_Wikipedia) Wikipedia [dataset hosted by OpenAI](https://cdn.openai.com/API/examples/data/vector_database_wikipedia_articles_embedded.zip) (~700MB zipped, 1.7GB CSV file) that includes vector embeddings. These are the columns of interest, where `content_vector` is a [vector embedding](/blog/vector-embeddings-explained) with [1536 elements (dimensions)](https://openai.com/blog/new-and-improved-embedding-model), generated using OpenAI's `text-embedding-ada-002` model:

| id | url | title | text | content_vector |
|----|-----|-------|------|----------------|
| 1 | https://simple<wbr/>.wikipedia.org<wbr/>/wiki/April | April | "April is the fourth month of the year..." | [-0.011034, -0.013401, ..., -0.009095] |

If you haven't already, make sure to download the dataset and unzip the file. You should end up with `vector_database_wikipedia_articles_embedded.csv` in your working directory. The records are mostly (but not strictly) sorted by title.

<p>
  <DownloadButton link="https://cdn.openai.com/API/examples/data/vector_database_wikipedia_articles_embedded.zip">Download Wikipedia dataset ZIP</DownloadButton>
</p>


## Create a Weaviate instance

We can create a Weaviate instance locally using the [embedded](../installation/embedded.md) option on Linux (transparent and fastest), Docker on any OS (fastest import and search), or in the cloud using the Weaviate Cloud (easiest setup, but importing may be slower due to the network speed). Each option is explained on its [Installation](../installation/index.md) page.

:::caution text2vec-openai
If using the Docker option, make sure to select "With Modules" (instead of standalone), and the `text2vec-openai` module when using the Docker configurator, at the "Vectorizer & Retriever Text Module" step. At the "OpenAI Requires an API Key" step, you can choose to "provide the key with each request", as we'll do so in the next section.
:::

## Connect to the instance and OpenAI

Add the OpenAI API key to the client so you can use the OpenAI vectorizer API when you send queries to Weaviate.

import ProvideOpenAIAPIKey from '/_includes/provide-openai-api-key-headers.mdx'

<ProvideOpenAIAPIKey />

## Create the schema

The [schema](../starter-guides/schema.md) defines the data structure for objects in a given Weaviate class. We'll create a schema for a Wikipedia `Article` class mapping the CSV columns, and using the [text2vec-openai vectorizer](../manage-data/collections.mdx#specify-a-vectorizer). The schema will have two properties:
* `title` - article title, not vectorized
* `content` - article content, corresponding to the `text` column from the CSV

As of Weaviate 1.18, the `text2vec-openai` vectorizer uses by default the same model as the OpenAI dataset, `text-embedding-ada-002`. To make sure the tutorial will work the same way if this default changes (i.e. if OpenAI releases an even better-performing model and Weaviate switches to it as the default), we'll configure the schema vectorizer explicitly to use the same model:

```json
{
  "moduleConfig": {
    "text2vec-openai": {
      "model": "ada",
      "modelVersion": "002",
      "type": "text"
    }
  }
}
```

Another detail to be careful about is how exactly we store the `content_vector` embedding. [Weaviate vectorizes entire objects](../config-refs/schema/index.md#configure-semantic-indexing) (not properties), and it includes by default the class name in the string serialization of the object it will vectorize. Since OpenAI has provided embeddings only for the `text` (content) field, we need to make sure Weaviate vectorizes an `Article` object the same way. That means we need to disable including the class name in the vectorization, so we must set `vectorizeClassName: false` in the `text2vec-openai` section of the `moduleConfig`. Together, these schema settings will look like this:

import CreateSchema from '/_includes/code/tutorials.wikipedia.schema.mdx';

<CreateSchema />

To quickly check that the schema was created correctly, you can navigate to `<weaviate-endpoint>/v1/schema`. For example in the Docker installation scenario, go to `http://localhost:8080/v1/schema` or run,

```bash
curl -s http://localhost:8080/v1/schema | jq
```

:::tip jq
The [`jq`](https://stedolan.github.io/jq/) command used after `curl` is a handy JSON preprocessor. When simply piping some text through it, `jq` returns the text pretty-printed and syntax-highlighted.
:::tip


## Import the articles

We're now ready to import the articles. For maximum performance, we'll load the articles into Weaviate via [batch import](../manage-data/import.mdx).

import ImportArticles from '/_includes/code/tutorials.wikipedia.import.mdx';

<ImportArticles />


### Checking the import went correctly

Two quick sanity checks that the import went as expected:

1. Get the number of articles
2. Get 5 articles

- Open the [Weaviate Query app](https://weaviate.io/developers/wcs/console)
- Connect to your Weaviate endpoint, either `http://localhost:8080` or `https://WEAVIATE_INSTANCE_URL`. (Replace WEAVIATE_INSTANCE_URL with your instance URL.)
- Run this GraphQL query:

```graphql
query {
  Aggregate { Article { meta { count } } }

  Get {
    Article(limit: 5) {
      title
      url
    }
  }
}
```

You should see the `Aggregate.Article.meta.count` field equal to the number of articles you've imported (e.g. 25,000), as well as five random articles with their `title` and `url` fields.


## Queries

Now that we have the articles imported, let's run some queries!

### nearText

The [`nearText` filter](../api/graphql/search-operators.md#neartext) lets us search for objects close (in vector space) to the vector representation of one or more concepts. For example, the vector for the query "modern art in Europe" would be close to the vector for the article [Documenta](https://simple.wikipedia.org/wiki/Documenta), which describes
> "one of the most important exhibitions of modern art in the world... [taking] place in Kassel, Germany".

import NearText from '/_includes/code/tutorials.wikipedia.nearText.mdx';

<NearText />

### hybrid

While `nearText` uses dense vectors to find objects similar in meaning to the search query, it does not perform very well on keyword searches. For example, a `nearText` search for "jackfruit" in this Simple English Wikipedia dataset, will find "cherry tomato" as the top result. For these (and indeed, most) situation, we can obtain better search results by using the [`hybrid` filter](../api/graphql/search-operators.md#hybrid), which combines dense vector search with keyword search:

import Hybrid from '/_includes/code/tutorials.wikipedia.hybrid.mdx';

<Hybrid />


## Recap

In this tutorial, we've learned
* how to efficiently import large datasets using Weaviate batching and CSV lazy loading with `pandas` / `csv-parser`
* how to import existing vectors ("Bring Your Own Vectors")
* how to quickly check that all records were imported
* how to use `nearText` and `hybrid` searches


## Suggested reading

- [Tutorial: Schemas in detail](../starter-guides/schema.md)
- [Tutorial: Queries in detail](./query.md)
- [Tutorial: Introduction to modules](./modules.md)



## Questions and feedback

import DocsFeedback from '/_includes/docs-feedback.mdx';

<DocsFeedback/>
