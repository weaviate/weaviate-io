---
title: "How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs"
slug: paper15
authors: [zain]
date: 2024-01-09
tags: []
image: ./img/hero.png
description: "Using persuasion ot jailbreak LLM's."
---
![A preview of the paper](./img/hero.png)

<!-- truncate -->

import { DownloadButton } from '/src/theme/Buttons';

üó£Ô∏èPersuasive Adversarial Prompting to Jailbreak LLMs with 92% Success Rate

üîíFascinating new paper breaks down jailbreak prompting to a science!

‚è©In Short:

1. Provide a taxonomy of 40 persuasion prompting techniques

2. Use this list of 40 techniques they can jailbreak LLMs including GPT4 with a 92% success rate!!

3. Pretty interestingly Anthropic models are not susceptible at all to PAP attacks!! More advanced models like GPT-4 are more vulnerable to persuasive adversarial prompts (PAPs).

4. If you can defend against these PAPs this also provides effective protection against other attacks

5. Test these PAPs to perform attacks covering 14 different risk categories (such as economic harm, etc.)

Blog+Demo: https://chats-lab.github.io/persuasive_jailbreaker/

<p>
  <DownloadButton link={require('./download/view.pdf').default}>Download the paper</DownloadButton>
</p>

<!-- We could create a specific template for Paper Review's -->
import WhatNext from '/_includes/what-next.mdx'

<WhatNext />
