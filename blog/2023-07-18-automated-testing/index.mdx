---
title: Better automated testing with Embedded Weaviate
slug: automated-testing
authors: [dan]
date: 2023-07-18
image: ./img/hero.png
tags: ['how-to']
description: "Learn how to make testing less of a chore with Embedded Weaviate, and other tips for better automated testing."

---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import FilteredTextBlock from '@site/src/components/Documentation/FilteredTextBlock';
import PyCode from '!!raw-loader!/_includes/code/automated-testing.py';
import TSCode from '!!raw-loader!/_includes/code/automated-testing.ts';
import { DownloadButton } from '/src/theme/Buttons';

![Automated testing for Weaviate applications](./img/hero.png)

As a software engineer with experience in test automation, I firmly believe in [Test-Driven Development](https://en.wikipedia.org/wiki/Test-driven_development), and more specifically, incorporating [integration testing](https://en.wikipedia.org/wiki/Integration_testing) from the very early stages of developing an application.

<!-- truncate -->

But you probably know that writing tests is quite a task in itself, and in many cases running them can also be a chore. For example, the test suite may need to set up and tear down a separate service such as a database, which can be time-consuming and error-prone.

I'm here to tell you that it doesn't have to be that way. In this article, I'll show you how to make testing easier with [Embedded Weaviate](/developers/weaviate/installation/embedded), and other tips for better automated testing.

And doing so, you might just discover the hidden gem that is the value provided by adding tests to your applications.

## Testing and Weaviate

In this article, we will focus on [integration tests](https://en.wikipedia.org/wiki/Integration_testing). Integrated testing is an important part of the development process, and especially so for complex applications. Weaviate-based apps usually fall in this category.

For one, Weaviate must interact with the application in a variety of ways. And additionally, Weaviate often interacts with external services such as vectorizers or LLMs.

Such complexity makes it important to test the application as a whole, and not just its individual components. This complexity also means that arranging the test suite can be cumbersome with a variety of moving parts that need to be set up and torn down.

[Embedded Weaviate](/developers/weaviate/installation/embedded) makes one part of this puzzle much easier, since Weaviate can be instantiated directly from the client. The following is all you need to do to start a Weaviate server:

<Tabs groupId="languages">
  <TabItem value="py" label="Python">
  <FilteredTextBlock
    text={PyCode}
    startMarker="# START Connect"
    endMarker="# Client is now ready to accept requests"
    language="py"
  />
  </TabItem>

  <TabItem value="js" label="JavaScript/TypeScript">
  <FilteredTextBlock
    text={TSCode}
    startMarker="// START Connect"
    endMarker="// Client is now ready to accept requests"
    language="js"
  />
  </TabItem>
</Tabs>

This is not only useful for new contributors to the project, but also for experienced developers. Starting anew as a new contributor, or working from a different machine on occasion, can be a hassle. With Embedded Weaviate, you can just run the test suite and be done with it.

But Embedded Weaviate is not the only way to make testing easier. In the following sections, we'll look at other ways to make testing easier, and how to make the most of Embedded Weaviate.


## Scoping tests

While you may be familiar with tests and integration tests in general, here are some specific suggestions for Weaviate-powered applications:
* **Whether to test search quality**: This depends primarily on the model used for vectorization, such as by a [Weaviate vectorizer module](/developers/weaviate/modules/retriever-vectorizer-modules). We suggest evaluating models separately, but not tested as a part of the application.
* **Focus on interactions with the inference provider**: Search itself is a core Weaviate functionality that we can trust. So, we suggest any integration tests focus on the interaction with the inference provider. For example,
  * is the vectorization model the expected one?
  * if switching to a different inference provider or model, does the application still function as expected?
* **Other common issues to test** include:
  * Connection or authentication issues with the inference provider
  * Incomplete or incorrect data imports
  * Specifying the vector correctly when [bringing your own vectors](/developers/weaviate/starter-guides/custom-vectors)
  * Data definition issues, like invalid class names, properties, or data types


## Testing with embedded Weaviate

### Set up

[Embedded Weaviate](/developers/weaviate/installation/embedded) lets us spawn a Weaviate server instance from the client, and automatically tear it down when the client terminates. The data is [persisted](/developers/weaviate/installation/embedded#lifecycle) between sessions, so we recommend deleting your data before each test.

Here's how to instantiate an embedded Weaviate server and perform this cleanup:

<details>
  <summary>Install dependencies</summary>

If you have yet to install the required dependencies, run the following command:

<Tabs groupId="languages">
  <TabItem value="py" label="Python">

  ```bash
  pip install -U weaviate-client pytest
  ```

  Then, save the code as `embedded_test.py` and run `pytest`.

  </TabItem>

  <TabItem value="js" label="JavaScript/TypeScript">

  ```bash
  npm install weaviate-ts-embedded typescript ts-node
  ```

  Then, save the code as `test.ts` and run `node --loader=ts-node/esm test.ts`:

  </TabItem>
</Tabs>

</details>

<Tabs groupId="languages">
  <TabItem value="py" label="Python">

  <FilteredTextBlock
    text={PyCode}
    startMarker="# START ConnectAndCleanup"
    endMarker="# Client connected and schema cleared"
    language="py"
  />
  </TabItem>

  <TabItem value="js" label="JavaScript/TypeScript">

  <FilteredTextBlock
    text={TSCode}
    startMarker="// START ConnectAndCleanup"
    endMarker="// Client connected and schema cleared"
    language="ts"
  />
  </TabItem>
</Tabs>

Now, let's walk through some examples of how you might construct integration tests with Embedded Weaviate.


### Our first test

As a simple example, let's create a class and then test that it was created correctly.

We'll create a class for question & answer objects from the game show *Jeopardy!*, by specifying its name and the vectorizer ([`text2vec-openai`](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-openai)).

Here, the integration test will consist of checking that the class was created with the expected default OpenAI vectorization model type, [`ada`](https://platform.openai.com/docs/guides/embeddings/embedding-models).

<Tabs groupId="languages">
  <TabItem value="py" label="Python">
    <FilteredTextBlock
      text={PyCode}
      startMarker="# Create the class"
      endMarker="# Class created successfully"
      language="py"
    />
  </TabItem>

  <TabItem value="js" label="JavaScript/TypeScript">
    <FilteredTextBlock
      text={TSCode}
      startMarker="// Create the class"
      endMarker="// Class created successfully"
      language="ts"
    />
  </TabItem>
</Tabs>

:::tip When might I use tests like these?
Although this is a simple test, you can imagine that if you have tens, or even hundreds of classes, tests like these can save you a lot of time and effort. And, if you're working with a team, you can be sure that everyone is on the same page about the expected schema.
:::

All we've done so far (instantiate and connect to Weaviate and OpenAI, perform cleanup, create the class and test creation), was done with very little code, thanks to Embedded Weaviate. Next, let's see how we might test imports.


### Testing imports

One particularly common issue we see is skipped objects during import due to rate limits from the vectorization provider. So, let's see how we might test that all objects were imported correctly.

In this section, we'll import a small subset (100 objects) of the [original Jeopardy dataset](https://www.kaggle.com/datasets/tunguz/200000-jeopardy-questions). As always, we'll use [batching](/developers/weaviate/manage-data/import) for optimal speed.

:::tip For very large files
While we load the JSON into memory here, you can use other methods such as [streaming](/developers/weaviate/manage-data/import#tip-stream-data-from-large-files) for very large JSON or CSV files.
:::

The test is simple; it verifies that all specified objects have been imported by performing an object count and checking it is 100.

<p>
  <DownloadButton link="https://raw.githubusercontent.com/weaviate-tutorials/edu-datasets/main/jeopardy_100.json">Download jeopardy_100.json</DownloadButton>
</p>


<Tabs groupId="languages">
  <TabItem value="py" label="Python">
    <FilteredTextBlock
      text={PyCode}
      startMarker="# Import objects from the JSON file"
      endMarker="# Import completed successfully"
      language="py"
    />
  </TabItem>

  <TabItem value="js" label="JavaScript/TypeScript">
    <FilteredTextBlock
      text={TSCode}
      startMarker="// Import objects from the JSON file"
      endMarker="// Import completed successfully"
      language="ts"
    />
  </TabItem>
</Tabs>

:::tip When might I use tests like these?
Such a test would provide a simple, repeatable, way of ensuring that all objects were imported correctly.
:::

And now that all data has been imported, let's see how we might test queries.

### Testing queries

Semantic (`nearText`) searches may be one of the most common (if not *the* most common) searches our users perform.

So let's see how we might test semantic searches. A semantic search requires vectorizing the query, so a test will validate the integration with the vectorizer (`text2vec-openai` in this case).

We'll run a query for "chemistry" and check that the top result is about "sodium".

:::info Will the top result always be the same?
Depending on your application and usage, you may find that the top result changes over time - for example, if your data changes over time. In this case, we will assume that the top result is immutable, or that you will update the specific test over time.
:::

<Tabs groupId="languages">
  <TabItem value="py" label="Python">
    <FilteredTextBlock
      text={PyCode}
      startMarker="# Run a test query"
      endMarker="# Query test completed"
      language="py"
    />
  </TabItem>

  <TabItem value="js" label="JavaScript/TypeScript">
    <FilteredTextBlock
      text={TSCode}
      startMarker="// Run a test query"
      endMarker="// Query test completed"
      language="ts"
    />
  </TabItem>
</Tabs>

:::tip When might I use tests like these?
A test like this can be used to ensure that the vectorizer is working as expected, and that the data has been imported correctly. For example - if the top result was something completely unintuitive and wildly dissimilar to the concept of "chemistry" - this might be cause to investigate further.

You could also test additional aspects, like the number of results returned, or the order of results.
:::

### End-to-end code

So far, we've seen how to test the following:
* Create the collection
* Import data
* Semantic search functionality

The code below brings together the setup and tests we've implemented so far - if you haven't done so yet, try running it yourself 😉.

<details>
  <summary>End-to-end code</summary>

<Tabs groupId="languages">
  <TabItem value="py" label="Python">
    <FilteredTextBlock
      text={PyCode}
      startMarker="# START Connect"
      endMarker="# END query"
      language="py"
    />
  </TabItem>

  <TabItem value="js" label="JavaScript/TypeScript">
    <FilteredTextBlock
      text={TSCode}
      startMarker="// START Connect"
      endMarker="// END all"
      language="ts"
    />
  </TabItem>
</Tabs>

</details>


## CI / CD

For many of us, Continuous Integration and Continuous Deployment (CI/CD) is a critical part of our development process. It allows us to automate the testing and deployment of our code, and to ensure that our code is always in a deployable state.

We use GitHub, and they offer GitHub Actions which we like at Weaviate. While we don't have space to cover it in detail (check out [their docs](https://docs.github.com/en/actions) if you're interested), we want to highlight that you can set up a YAML file to automate running of tests on GitHub. The YAML file could look something like this:

<Tabs groupId="languages">
  <TabItem value="py" label="Python">

```
name: Run Python Automated Tests
on: [push]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v2
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: 3.11
    - name: Install dependencies
      run: pip install weaviate-client pytest
    - name: Run tests
      run: pytest
```

  </TabItem>

  <TabItem value="js" label="JavaScript/TypeScript">

```
name: Run Node.js Automated Tests
on: [push]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    - name: Use Node.js 18.x
      uses: actions/setup-node@v3
      with:
        node-version: 18.x
        cache: 'npm'
    - run: npm ci
    - run: npm run build --if-present
    - run: node --loader=ts-node/esm test.ts
```

  </TabItem>
</Tabs>

With this step, you'll be able to run your tests on every push to the main branch.

And did you notice that we didn't have to spin up an instance of Weaviate through GitHub Actions? That's because we're using Embedded Weaviate, which means we can run our tests without worrying about the Weaviate server instance.

This might not seem like a big deal, but it lowers the barrier to entry for running tests, and makes it easier to run tests locally, or in a CI/CD pipeline.

As a result, your application will be more robust, and you'll be able to deploy with confidence.


## Closing thoughts

In this post, we've seen how to write an integration test for an application using Weaviate &mdash; [Embedded Weaviate](/developers/weaviate/installation/embedded) in particular.

With just a few lines of code, we are able to verify how we import a data set, vectorize it, then export the vectorized objects. The test can be extended with search, insertion, updates, deletes, and other operations that are part of the user journey.

What's more - because we were using Embedded Weaviate, the journey from the start to finish was far easier, not to mention portable.

So what are you waiting for? Try out Embedded Weaviate - and add those tests to your application that you've been putting off 😉.

<hr/>

What other aspects of integration testing would you like to learn about? Let us know in the comments below!


import WhatNext from  '/_includes/what-next.mdx'

<WhatNext />
