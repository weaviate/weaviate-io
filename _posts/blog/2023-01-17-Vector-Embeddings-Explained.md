---
layout: post
title: Vector Embeddings Explained
description: "Learn more about vector embeddings and how they are generated."
published: true
author: Dan Dascalescu 
author-img: /img/people/Dan.jpg
card-img: /img/blog/hero/vector-embeddings-explained.png
hero-img: /img/blog/hero/vector-embeddings-explained.png
og: /img/blog/hero/vector-embeddings-explained.png
date: 2023-01-16
toc: true
---

One of the most common use cases for Weaviate is to perform semantic searches, for example using the [`hybrid`](/developers/weaviate/current/graphql-references/vector-search-parameters.html#hybrid), [`nearText`](/developers/weaviate/current/tutorials/how-to-perform-a-semantic-search.html#neartext-filter), or [`ask`](/developers/weaviate/current/graphql-references/vector-search-parameters.html#ask) operators. If you’ve ever wondered how that works under the hood, this article is for you.

![vector embeddings example](/img/blog/vector-embeddings-explained/vector-embeddings-example.png)

Semantic searches (as well as question answering) are essentially searches by similarity, such as by the meaning of text, or by what objects are contained in images. For example, consider a library of wine names and descriptions, one of which mentioning that the wine is “good with **fish**”. A “wine for **seafood**” keyword search, or even a synonym search, won’t find that wine. A meaning-based search should understand that “fish” is a type of “seafood”, and should find the wine.

How can computers mimic our understanding of language, and similarities of words or paragraphs? To tackle this problem, semantic search uses at its core a data structure called **vector embedding** (or simply, **vector** or **embedding**), which is an array of numbers. A [vector search engine](/blog/2022/12/vector-library-vs-vector-database.html) like Weaviate will compute an embedding for each data object as it is inserted or updated into the database. The embeddings are placed into an index, so that the search engine can [quickly](/blog/2022/09/Why-is-Vector-Search-so-fast.html) find the [closest](/blog/2022/09/Distance-Metrics-in-Vector-Search.html) vectors to a given vector that will be computed for the query.

## What exactly are vector embeddings?
Vectors are numeric representations of data that capture certain features of the data. For example, in the case of text data, “cat” and “kitty” have similar meaning, even though the _words_ “cat” and “kitty” are very different if compared letter by letter. For semantic search to work effectively, representations of “cat” and “kitty” must sufficiently capture their semantic similarity. This is where vector representations are used, and why their derivation is so important.

In practice, vectors are arrays of real numbers, of a fixed length (typically from hundreds to thousands of elements), generated by machine learning models. The process of generating a vector for a data object is called vectorization. Weaviate generates vector embeddings using [modules](/developers/weaviate/current/modules/index.html), and conveniently stores both objects and vectors in the same database. For example, vectorizing the two words above might result in:

```text
cat = [1.5, -0.4, 7.2, 19.6, 3.1, ..., 20.2]
kitty = [1.5, -0.4, 7.2, 19.5, 3.2, ..., 20.8]
```

These two vectors have a very high similarity. In contrast, vectors for “banjo”, or “comedy” would not be very similar to either of these vectors. To this extent, vectors capture the semantic similarity of words.

Now that you’ve seen what vectors are, and that they can represent meaning to some extent, you might have further questions. For one, what does each number represent? That depends on the machine learning model that generated the vectors, and isn’t necessarily clear at least in terms of our human conception of language and meaning. But we can sometimes gain a rough idea by correlating vectors to words with which we are familiar. 

Vector-based representation of meaning caused quite a [stir](https://www.ed.ac.uk/informatics/news-events/stories/2019/king-man-woman-queen-the-hidden-algebraic-struct) a few years back, with the revelation of mathematical operations between words. Perhaps *the* most famous result is that of “king − man + woman ≈ queen”, indicating that the difference between “king” and “man” was some sort of “royalty”, which was analogously and mathematically applicable to “queen” minus “woman”. Jay Alamar provided a helpful [visualization](https://jalammar.github.io/illustrated-word2vec/) around this equation:

![vector embeddings visualization](/img/blog/vector-embeddings-explained/vector-embeddings-visualization.png)

In the example above, the concepts are vectorized into (represented by) an array of 50 numbers generated using the [GloVe model](https://en.wikipedia.org/wiki/GloVe), and those numbers are visualized using colors. In [vector terminology](https://en.wikipedia.org/wiki/Vector_(mathematics)), the 50 numbers are called dimensions. We can see that all words share a dark blue column in one of the dimensions (though we can’t quite tell what that represents), and the word “water” _looks_ quite different from the rest, which makes sense given that the rest are people. Also, “girl” and “boy” look more similar to each other than to “king” and “queen” respectively, while “king” and “queen” look similar to each other as well.

So we can **see** that these vector embeddings of words align with our intuitive understanding of meaning. And even more amazingly, vector embeddings are not limited to representing meanings of words.

In fact, effective vector embeddings can be generated from any kind of data object. Text is the most common, followed by images, then audio (this is how Shazam recognizes songs based on a short and even noisy audio clip), but also time series data, 3D models, video, molecules etc. Embeddings are generated such that two objects with similar semantics will have vectors that are "close" to each other, i.e. that have a "small" distance between them in [vector space](https://en.wikipedia.org/wiki/Vector_space_model). That [distance can be calculated in multiple ways](/blog/2022/09/Distance-Metrics-in-Vector-Search.html), one of the simplest being "The sum of the absolute differences between elements at position i in each vector" (recall that all vectors have the same fixed length).

Let's look at some numbers (no math, promise!) and illustrate with another text example:

Objects (data): words including `cat`, `dog`, `apple`, `strawberry`, `building`, `car`

Search query: `fruit`

A set of simplistic vector embeddings (with only 5 dimensions) for the objects and the query could look something like this:

| Word         | Vector embedding                |
|--------------|---------------------------------|
| `cat`        | `[1.5, -0.4, 7.2, 19.6, 20.2]`  |
| `dog`        | `[1.7, -0.3, 6.9, 19.1, 21.1]`  |
| `apple`      | `[-5.2, 3.1, 0.2, 8.1, 3.5]`    |
| `strawberry` | `[-4.9, 3.6, 0.9, 7.8, 3.6]`    |
| `building`   | `[60.1, -60.3, 10, -12.3, 9.2]` |
| `car`        | `[81.6, -72.1, 16, -20.2, 102]` |
| Q: `fruit`   | `[-5.1, 2.9, 0.8, 7.9, 3.1]`    |

If we look at each of the 5 elements of the vectors, we can see quickly that `cat` and `dog` are much closer than `dog` and `apple` (we don’t even need to calculate the distances). In the same way, `fruit` is much closer to `apple` and `strawberry` than to the other words, so those will be the top results of the “fruit” query.

But where do these numbers come from? That’s where the real magic is, and where advances in modern deep learning have made a huge impact. 

## How are vector embeddings generated?
The magic of vector search resides primarily in how the embeddings are generated for each entity and the query, and secondarily in how to efficiently search within very large datasets (see our [“Why is Vector Search so Fast”](/blog/2022/09/Why-is-Vector-Search-so-fast.html) article for the latter).

As we mentioned, vector embeddings can be generated for various media types such as text, images, audio and others. For text, vectorization techniques have evolved tremendously over the last decade, from inefficient methods like “[One-Hot Encoding](​​https://www.tensorflow.org/text/guide/word_embeddings#one-hot_encodings)”, to the venerable [word2vec](https://en.wikipedia.org/wiki/Word2vec) (2013), to the state of the art transformer models, such as [BERT](https://en.wikipedia.org/wiki/BERT_(language_model)), published [in 2018 by Google](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html).

Vectorization of text objects begins with tokenization: the text is lowercased, split into tokens such as words, stop words (“the”, “a” etc.) (and often punctuation) are removed, and rare words are replaced with placeholders. The resulting tokens are then encoded using one of the methods mentioned above. Finally, the encodings are combined into a sentence-level encoding, e.g. by averaging the values for all words across each dimension.

### One-Hot Encoding
One-Hot Encoding is an older data representation technique. It calculates the dimensionality of the vector as the size of the vocabulary (top N most frequent words), then assigns to each word a vector with N-1 zeros and a `1` at the index of that word in the vocabulary. Since the size of the vocabulary often greatly outweighs words used in a particular text, they tend to produce vectors that mostly contain zeros. For this reason, these vectors are called sparse. The One-Hot Encoding method consequently consumes large amounts of storage and suffers from a number of limitations, mainly that the order of words is ignored, and that the algorithm only matches exact words (“run” and ”ran” are completely different to it, even though to humans they’re very close).

### Word-level dense vector models (word2vec, GloVe, etc.)
[word2vec](https://wiki.pathmind.com/word2vec) is a [family of model architectures](https://www.tensorflow.org/tutorials/text/word2vec) that introduced the idea of “dense” vectors in language processing, in which all values are non-zero.

Word2vec in particular uses a neural network [model](https://arxiv.org/pdf/1301.3781.pdf) to learn word associations from a large corpus of text (it was initially trained by Google with 100 billion words). It has some advantages over One-Hot Encoding:
* the meaning of words can be quantified - “run” and “ran” are no longer completely different words, and the model recognizes that they are far more similar than “run” and “coffee”
* word vectors can be manipulated to derive analogies (as in the “king is to man as is queen is to…” example you saw earlier), and
* words vectors can be aggregated to represent larger strings 

As the name suggests, word2vec is a word-level model and cannot by itself produce a vector to represent longer text such as sentences, paragraphs or documents. However, this can be done by aggregating vectors of constituent words, which is often done by incorporating weightings such that certain words are weighted more heavily than others.

However, word2vec still suffers from important limitations:
* It doesn’t address words with multiple meanings (polysemantic): “run”, “set”, “go”, or “take” each have [over 300 meanings](https://www.insider.com/words-with-the-most-definitions-2019-1) (!)
* It doesn’t address words with ambiguous meanings: “to consult” can be its own antonym, like [many other words](http://www.dailywritingtips.com/75-contronyms-words-with-contradictory-meanings/)

Which takes us to the next, state-of-the-art, models.

### Transformer models (BERT, ELmo, and others)
The current state-of-the-art models are based on what’s called a “transformer” architecture as introduced in [this paper](https://arxiv.org/abs/1706.03762).  

[Transformer models](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)) such as BERT and its successors improve search accuracy, [precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall) by looking at every word’s context to create full contextual embeddings (though [the exact mechanism of BERT’s success is not fully understood](https://aclanthology.org/D19-1445/)). Unlike word2vec embeddings which are context-agnostic, transformer-generated embeddings take the entire input text into account—each occurrence of a word has its own embedding that is modified by the surrounding text. These embeddings better reflect the polysemantic nature of words, which can only be disambiguated when they are considered in context. 

Some of the potential downsides include:
* increased compute requirements: fine-tuning transformer models is much slower (on the order of hours vs. minutes)
* increased memory requirements: context-sensitivity greatly increases memory requirements, which often leads to limited possible input lengths

Despite these downsides, transformer models have been wildly successful. Countless text vectorizer models have proliferated over the recent past. Plus, many more vectorizer models exist for other data types such as audio, video and images, to name a few. Some models, such as CLIP, are capable of vectorizing multiple data types (images and text in this case) into one vector space, so that an image can be searched by its content using only text.

#### Vector embeddings with Weaviate

For this reason, Weaviate is configured to support many different vectorizer models and vectorizer service providers. You can even bring your own vectors, for example if you already have a vectorization pipeline available, or if none of the publicly available models are suitable for you.

For one, Weaviate supports using any Hugging Face models through our text2vec-hugginface module, so that you can [choose one of the many sentence transformers published on Hugging Face](/blog/2022/10/How-to-Choose-a-Sentence-Transformer-from-Hugging-Face.html). Or, you can use other very popular vectorization APIs such as OpenAI or Cohere through the [`text2vec-openai`](/developers/weaviate/current/retriever-vectorizer-modules/text2vec-openai.html) or [`text2vec-cohere`](/developers/weaviate/current/retriever-vectorizer-modules/text2vec-cohere.html) modules. You can even run transformer models locally with [`text2vec-transformers`](/developers/weaviate/current/retriever-vectorizer-modules/text2vec-transformers.html), and modules such as [`multi2vec-clip`](/developers/weaviate/current/retriever-vectorizer-modules/multi2vec-clip.html) can convert images and text to vectors using a CLIP model. 

But they all perform the same core task—which is to represent the “meaning” of the original data as a set of numbers. And that’s why semantic search works so well. 

## Stay Connected
Thank you so much for reading! If you would like to talk to us more about this topic, please connect with us on [Slack](https://join.slack.com/t/weaviate/shared_invite/zt-goaoifjr-o8FuVz9b1HLzhlUfyfddhw){:target="_blank"} or [Twitter](https://twitter.com/weaviate_io){:target="_blank"}. 

Weaviate is open-source, you can follow the project on [GitHub](https://github.com/semi-technologies/weaviate){:target="_blank"}. Don't forget to give us a ⭐️ while you are there.